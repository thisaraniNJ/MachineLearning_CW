{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fGapFmI18pXDQyZkjQoih2U8F_eJxFCZ",
      "authorship_tag": "ABX9TyPpqp5P3MOIQX5t7rQG2Gvx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thisaraniNJ/MachineLearning_CW/blob/main/MachineLearningCW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n"
      ],
      "metadata": {
        "id": "T4n0xhWkMtMd"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Cleaning Process**\n"
      ],
      "metadata": {
        "id": "Sv9udeiKa_i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the CSV file using semicolon as the delimiter\n",
        "df = pd.read_csv('/content/drive/MyDrive/ML CW/bank-full.csv', delimiter=';')\n",
        "\n",
        "# Display the first few rows to check the data\n",
        "print(df.head())\n",
        "\n",
        "# Manually set the correct column names\n",
        "columns = [\n",
        "    'age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan',\n",
        "    'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'y'\n",
        "]\n",
        "\n",
        "# Ensure the DataFrame has the correct number of columns before setting\n",
        "if len(df.columns) == len(columns):\n",
        "    df.columns = columns\n",
        "else:\n",
        "    print(f\"Warning: Column count mismatch. Found {len(df.columns)} columns, expected {len(columns)}.\")\n",
        "\n",
        "# Replace 'unknown' with NaN for easier handling\n",
        "df.replace('unknown', pd.NA, inplace=True)\n",
        "\n",
        "# List of categorical columns to process\n",
        "categorical_cols = [\"job\", \"marital\", \"education\", \"contact\", \"poutcome\"]\n",
        "\n",
        "for col in categorical_cols:\n",
        "    # Check if the column exists before imputing\n",
        "    if col in df.columns:\n",
        "        mode_value = df[col].mode()[0]  # Get the mode value explicitly\n",
        "        df.loc[:, col] = df[col].fillna(mode_value)  # Use df.loc for assignment\n",
        "    else:\n",
        "        print(f\"Warning: Column '{col}' not found in DataFrame\")\n",
        "\n",
        "# Handle 'pdays' column (replace -1 with 0 for meaningful representation)\n",
        "if 'pdays' in df.columns:\n",
        "    df.loc[:, 'pdays'] = df['pdays'].apply(lambda x: 0 if x == -1 else x)\n",
        "else:\n",
        "    print(\"Warning: Column 'pdays' not found in DataFrame\")\n",
        "\n",
        "# Encode target variable 'y' (convert 'yes'/'no' to 1/0)\n",
        "if 'y' in df.columns:\n",
        "    # Replace 'unknown' in 'y' column with mode or specific value before mapping:\n",
        "    mode_value_y = df['y'].mode()[0]  # Get the mode of 'y' column\n",
        "    df.loc[:, 'y'] = df['y'].replace('unknown', mode_value_y)  # Replace 'unknown' with mode\n",
        "\n",
        "    # Now map to numerical values\n",
        "    df.loc[:, 'y'] = df['y'].map({'yes': 1, 'no': 0}).astype(int)\n",
        "else:\n",
        "    print(\"Warning: Column 'y' (target) not found in DataFrame\")\n",
        "\n",
        "# Display cleaned dataset info\n",
        "print(\"\\nCleaned Dataset Info:\")\n",
        "print(df.info())\n",
        "print(\"\\nCleaned Dataset:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjHDCtlfM4Ux",
        "outputId": "3e2dbd7f-d9eb-42e0-d84d-6c4769e332b5"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age           job  marital  education default  balance housing loan  \\\n",
            "0   58    management  married   tertiary      no     2143     yes   no   \n",
            "1   44    technician   single  secondary      no       29     yes   no   \n",
            "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
            "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
            "4   33       unknown   single    unknown      no        1      no   no   \n",
            "\n",
            "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
            "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
            "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
            "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
            "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
            "4  unknown    5   may       198         1     -1         0  unknown  no  \n",
            "\n",
            "Cleaned Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 45211 entries, 0 to 45210\n",
            "Data columns (total 17 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   age        45211 non-null  int64 \n",
            " 1   job        45211 non-null  object\n",
            " 2   marital    45211 non-null  object\n",
            " 3   education  45211 non-null  object\n",
            " 4   default    45211 non-null  object\n",
            " 5   balance    45211 non-null  int64 \n",
            " 6   housing    45211 non-null  object\n",
            " 7   loan       45211 non-null  object\n",
            " 8   contact    45211 non-null  object\n",
            " 9   day        45211 non-null  int64 \n",
            " 10  month      45211 non-null  object\n",
            " 11  duration   45211 non-null  int64 \n",
            " 12  campaign   45211 non-null  int64 \n",
            " 13  pdays      45211 non-null  int64 \n",
            " 14  previous   45211 non-null  int64 \n",
            " 15  poutcome   45211 non-null  object\n",
            " 16  y          45211 non-null  object\n",
            "dtypes: int64(7), object(10)\n",
            "memory usage: 5.9+ MB\n",
            "None\n",
            "\n",
            "Cleaned Dataset:\n",
            "   age           job  marital  education default  balance housing loan  \\\n",
            "0   58    management  married   tertiary      no     2143     yes   no   \n",
            "1   44    technician   single  secondary      no       29     yes   no   \n",
            "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
            "3   47   blue-collar  married  secondary      no     1506     yes   no   \n",
            "4   33   blue-collar   single  secondary      no        1      no   no   \n",
            "\n",
            "    contact  day month  duration  campaign  pdays  previous poutcome  y  \n",
            "0  cellular    5   may       261         1      0         0  failure  0  \n",
            "1  cellular    5   may       151         1      0         0  failure  0  \n",
            "2  cellular    5   may        76         1      0         0  failure  0  \n",
            "3  cellular    5   may        92         1      0         0  failure  0  \n",
            "4  cellular    5   may       198         1      0         0  failure  0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering"
      ],
      "metadata": {
        "id": "iAgFz4cXlEFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: Create new features (optional, based on the dataset)\n",
        "# Create 'age_group' based on age ranges\n",
        "df['age_group'] = pd.cut(df['age'], bins=[0, 25, 50, 75, 100], labels=['0-25', '26-50', '51-75', '76+'])\n",
        "\n",
        "# Create 'balance_category' based on account balance\n",
        "df['balance_category'] = pd.cut(df['balance'], bins=[-float('inf'), 0, 1000, 10000, float('inf')],\n",
        "                                labels=['low', 'medium', 'high', 'very_high'])\n",
        "\n",
        "# Create 'duration_group' based on the duration of the last contact\n",
        "df['duration_group'] = pd.cut(df['duration'], bins=[0, 300, 600, 1200, float('inf')],\n",
        "                              labels=['short', 'medium', 'long', 'very_long'])\n",
        "\n",
        "# Step 2: Select features to use in the model\n",
        "# We will drop the original columns that were transformed (e.g., 'age', 'balance', 'duration') and use the new features\n",
        "features_to_use = df.drop(columns=['y', 'age', 'balance', 'duration'])\n",
        "\n",
        "# Step 3: Split the data into features (X) and target variable (y)\n",
        "X = features_to_use\n",
        "y = df['y']\n",
        "\n",
        "# Step 4: Preprocessing pipeline for numerical and categorical features\n",
        "# Define the numerical features (those that are not categorical)\n",
        "numerical_cols = ['campaign', 'pdays', 'previous', 'day']\n",
        "\n",
        "# Preprocessing for numerical data: Scaling using StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Numerical transformation pipeline\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean\n",
        "    ('scaler', StandardScaler())  # Scale features to have mean=0 and variance=1\n",
        "])\n",
        "\n",
        "# Preprocessing for categorical data: One-hot encoding\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "categorical_cols = ['job', 'marital', 'education', 'contact', 'poutcome', 'default', 'housing', 'loan', 'month',\n",
        "                    'age_group', 'balance_category', 'duration_group']  # Updated list\n",
        "\n",
        "# Categorical transformation pipeline\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent category\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical variables\n",
        "])\n",
        "\n",
        "# Combine both transformations using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),  # Using updated numerical_cols\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Step 5: Fit the preprocessor on the data and transform it\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Step 6: View the transformed data (optional)\n",
        "print(f\"\\nTransformed Data Shape: {X_processed.shape}\")\n",
        "print(\"\\nTransformed Data (First 5 Rows):\")\n",
        "print(X_processed[:5])\n",
        "\n",
        "# You now have your features preprocessed and ready to be used in a machine learning model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spoM2Nedk_te",
        "outputId": "b824f0df-974d-4a57-b9d7-ad6f1f4484a7"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformed Data Shape: (45211, 56)\n",
            "\n",
            "Transformed Data (First 5 Rows):\n",
            "  (0, 0)\t-0.5693506376457914\n",
            "  (0, 1)\t-0.41100886034711376\n",
            "  (0, 2)\t-0.25194037067217256\n",
            "  (0, 3)\t-1.2984763315738133\n",
            "  (0, 8)\t1.0\n",
            "  (0, 16)\t1.0\n",
            "  (0, 20)\t1.0\n",
            "  (0, 21)\t1.0\n",
            "  (0, 23)\t1.0\n",
            "  (0, 26)\t1.0\n",
            "  (0, 29)\t1.0\n",
            "  (0, 30)\t1.0\n",
            "  (0, 40)\t1.0\n",
            "  (0, 46)\t1.0\n",
            "  (0, 48)\t1.0\n",
            "  (0, 54)\t1.0\n",
            "  (1, 0)\t-0.5693506376457914\n",
            "  (1, 1)\t-0.41100886034711376\n",
            "  (1, 2)\t-0.25194037067217256\n",
            "  (1, 3)\t-1.2984763315738133\n",
            "  (1, 13)\t1.0\n",
            "  (1, 17)\t1.0\n",
            "  (1, 19)\t1.0\n",
            "  (1, 21)\t1.0\n",
            "  (1, 23)\t1.0\n",
            "  :\t:\n",
            "  (3, 21)\t1.0\n",
            "  (3, 23)\t1.0\n",
            "  (3, 26)\t1.0\n",
            "  (3, 29)\t1.0\n",
            "  (3, 30)\t1.0\n",
            "  (3, 40)\t1.0\n",
            "  (3, 45)\t1.0\n",
            "  (3, 48)\t1.0\n",
            "  (3, 54)\t1.0\n",
            "  (4, 0)\t-0.5693506376457914\n",
            "  (4, 1)\t-0.41100886034711376\n",
            "  (4, 2)\t-0.25194037067217256\n",
            "  (4, 3)\t-1.2984763315738133\n",
            "  (4, 5)\t1.0\n",
            "  (4, 17)\t1.0\n",
            "  (4, 19)\t1.0\n",
            "  (4, 21)\t1.0\n",
            "  (4, 23)\t1.0\n",
            "  (4, 26)\t1.0\n",
            "  (4, 28)\t1.0\n",
            "  (4, 30)\t1.0\n",
            "  (4, 40)\t1.0\n",
            "  (4, 45)\t1.0\n",
            "  (4, 50)\t1.0\n",
            "  (4, 54)\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Check the shapes of the train and test sets\n",
        "print(f\"\\nTraining Data Shape: {X_train.shape}\")\n",
        "print(f\"Testing Data Shape: {X_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC0TW0jmq1_l",
        "outputId": "a2a76042-9d49-4462-90fc-2bc35e5422ed"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Data Shape: (31647, 56)\n",
            "Testing Data Shape: (13564, 56)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique values in y_train:\", y_train.unique())\n",
        "print(\"Data type of y_train:\", y_train.dtype)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lscCvz10q3Ia",
        "outputId": "076a56f4-dd1a-4e1c-d1c0-49b1a9203553"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in y_train: [0 1]\n",
            "Data type of y_train: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode 'yes' as 1 and 'no' as 0\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n"
      ],
      "metadata": {
        "id": "V8wxsCIkInmg"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with encoded labels\n",
        "rf_model.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_rf_prob = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Random Forest Model Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_encoded, y_pred_rf))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_encoded, y_pred_rf))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test_encoded, y_pred_rf_prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiSSyj6jIq2q",
        "outputId": "cc77489d-892a-437a-cc70-a4a59068d108"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Model Results:\n",
            "Accuracy: 0.894426422884105\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.96      0.94     11966\n",
            "           1       0.58      0.39      0.46      1598\n",
            "\n",
            "    accuracy                           0.89     13564\n",
            "   macro avg       0.75      0.67      0.70     13564\n",
            "weighted avg       0.88      0.89      0.89     13564\n",
            "\n",
            "ROC-AUC Score: 0.8931862011201116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder # Import LabelEncoder\n",
        "\n",
        "\n",
        "# Define the Neural Network structure\n",
        "model_nn = Sequential([\n",
        "    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer\n",
        "    Dense(16, activation='relu'),  # Hidden layer\n",
        "    Dense(1, activation='sigmoid')  # Output layer (binary classification)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Add early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "# Convert y_train to float32\n",
        "y_train_float = y_train.astype(np.float32)\n",
        "history = model_nn.fit(\n",
        "    X_train, y_train_float, # Use the converted y_train\n",
        "    epochs=50,  # Maximum number of epochs\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,  # Use 20% of training data for validation\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "# Convert y_test to float32 as well\n",
        "y_test_float = y_test.astype(np.float32)\n",
        "loss, accuracy = model_nn.evaluate(X_test, y_test_float) # Use the converted y_test\n",
        "y_pred_nn_prob = model_nn.predict(X_test).flatten()\n",
        "y_pred_nn = (y_pred_nn_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "\n",
        "# Encode y_test using the same LabelEncoder used for y_train\n",
        "# Assuming you have a label_encoder object from previous code\n",
        "label_encoder = LabelEncoder() # Initialize or retrieve if it already exists\n",
        "y_test_encoded = label_encoder.fit_transform(y_test) # Fit and transform or only transform if already fit\n",
        "\n",
        "\n",
        "# Neural Network Results\n",
        "print(\"\\nNeural Network Model Results:\")\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_encoded, y_pred_nn)) # Use encoded y_test\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test_encoded, y_pred_nn_prob)) # Use encoded y_test\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wr8aU5jq5_N",
        "outputId": "ab1c3177-b2a9-405c-c4e8-5055a192f4e0"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8474 - loss: 0.3523 - val_accuracy: 0.8994 - val_loss: 0.2347\n",
            "Epoch 2/50\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8962 - loss: 0.2404 - val_accuracy: 0.8987 - val_loss: 0.2313\n",
            "Epoch 3/50\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8982 - loss: 0.2400 - val_accuracy: 0.8997 - val_loss: 0.2283\n",
            "Epoch 4/50\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9009 - loss: 0.2308 - val_accuracy: 0.9006 - val_loss: 0.2251\n",
            "Epoch 5/50\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8998 - loss: 0.2311 - val_accuracy: 0.9030 - val_loss: 0.2237\n",
            "Epoch 6/50\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9059 - loss: 0.2211 - val_accuracy: 0.9017 - val_loss: 0.2241\n",
            "Epoch 7/50\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9062 - loss: 0.2174 - val_accuracy: 0.8994 - val_loss: 0.2224\n",
            "Epoch 8/50\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9061 - loss: 0.2184 - val_accuracy: 0.9009 - val_loss: 0.2235\n",
            "Epoch 9/50\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9076 - loss: 0.2117 - val_accuracy: 0.9013 - val_loss: 0.2223\n",
            "Epoch 10/50\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9098 - loss: 0.2098 - val_accuracy: 0.9032 - val_loss: 0.2192\n",
            "Epoch 11/50\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9112 - loss: 0.2044 - val_accuracy: 0.9003 - val_loss: 0.2209\n",
            "Epoch 12/50\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9074 - loss: 0.2118 - val_accuracy: 0.9009 - val_loss: 0.2229\n",
            "Epoch 13/50\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9090 - loss: 0.2035 - val_accuracy: 0.9019 - val_loss: 0.2248\n",
            "Epoch 14/50\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9116 - loss: 0.2049 - val_accuracy: 0.9044 - val_loss: 0.2214\n",
            "Epoch 15/50\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9122 - loss: 0.2047 - val_accuracy: 0.9017 - val_loss: 0.2217\n",
            "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8995 - loss: 0.2355\n",
            "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "Neural Network Model Results:\n",
            "Test Accuracy: 0.9010616540908813\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.97      0.95     11966\n",
            "           1       0.62      0.42      0.50      1598\n",
            "\n",
            "    accuracy                           0.90     13564\n",
            "   macro avg       0.77      0.69      0.72     13564\n",
            "weighted avg       0.89      0.90      0.89     13564\n",
            "\n",
            "ROC-AUC Score: 0.9034187289518886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-STbag6V_47z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "neSKANAVM36B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hScCzjejM3v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aw7XqeWNM3lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fFgeHwLMM3YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lkp0anwLM28f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}